{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIC-IDS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL CODE IS MINE UNLESS OTHERWISE STATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, BatchNormalization, Input, Add, GlobalAveragePooling1D, ReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    return max(lr * 0.95, 1e-6) # Prevents learning rate from going too low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df_list = []\n",
    "dataset_files = [\n",
    "    \"data/CIC-2017/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",\n",
    "    \"data/CIC-2017/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
    "    \"data/CIC-2017/Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
    "    \"data/CIC-2017/Monday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"data/CIC-2017/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\",\n",
    "    \"data/CIC-2017/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all datasets\n",
    "for file in dataset_files:\n",
    "    df_list.append(pd.read_csv(file))\n",
    "df = pd.concat(df_list)\n",
    "df = df.rename(columns={\" Label\": \"Label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df['Label'].apply(lambda x: 0 if x.strip() == \"BENIGN\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing and infinite values\n",
    "df.replace('Infinity', -1, inplace=True)\n",
    "df.replace([np.inf, -np.inf], -1, inplace=True)\n",
    "df.fillna(df.max().max(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoders.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding\n",
    "label_encoders = {}\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "joblib.dump(feature_names, 'features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "joblib.dump(scaler, 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE for class balancing\n",
    "smote = SMOTE(sampling_strategy=0.3, random_state=42)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical\n",
    "y_train_bal = to_categorical(y_train_bal)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for CNN\n",
    "X_train_reshaped = X_train_bal.reshape((X_train_bal.shape[0], X_train_bal.shape[1], 1))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters):\n",
    "    shortcut = x\n",
    "    # Apply Convolution, Batch Normalization, and ReLU\n",
    "    x = Conv1D(filters, kernel_size=3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = Conv1D(filters, kernel_size=3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Adjust shortcut to match the shape if needed\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, kernel_size=1, padding='same')(shortcut)\n",
    "\n",
    "    # Add residual connection\n",
    "    x = Add()([x, shortcut])\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN Model\n",
    "def build_cnn_model(hp):\n",
    "    inputs = Input(shape=(X_train_reshaped.shape[1], 1))\n",
    "    \n",
    "    # First Conv Layer\n",
    "    x = Conv1D(hp.Int('filters_1', 64, 256, step=64), kernel_size=3, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Residual blocks\n",
    "    for i in range(hp.Int('num_res_blocks', 1, 3)):\n",
    "        x = residual_block(x, filters=hp.Int(f'filters_res_{i}', 64, 256, step=64))\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Dense Layer\n",
    "    x = Dense(hp.Int('dense_units', 64, 256, step=64), activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[0.001, 0.0001])),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hyperparameter Tuning with Pruning...\n",
      "Reloading Tuner from cnn_tuner\\CIC_IDS_Tuning\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Tune model\n",
    "print(\"Starting Hyperparameter Tuning with Pruning...\")\n",
    "tuner = kt.Hyperband(\n",
    "    build_cnn_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory='cnn_tuner',\n",
    "    project_name='CIC_IDS_Tuning',\n",
    "    executions_per_trial=1\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "tuner.search(X_train_reshaped, y_train_bal, epochs=20, validation_split=0.2, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Best Model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_cnn_model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 10ms/step - accuracy: 0.9651 - loss: 0.0927 - val_accuracy: 0.9754 - val_loss: 0.0706\n",
      "Epoch 2/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 10ms/step - accuracy: 0.9787 - loss: 0.0529 - val_accuracy: 0.9676 - val_loss: 0.1069\n",
      "Epoch 3/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 10ms/step - accuracy: 0.9801 - loss: 0.0490 - val_accuracy: 0.9182 - val_loss: 0.1506\n",
      "Epoch 4/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 11ms/step - accuracy: 0.9810 - loss: 0.0466 - val_accuracy: 0.9741 - val_loss: 0.0650\n",
      "Epoch 5/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 11ms/step - accuracy: 0.9814 - loss: 0.0444 - val_accuracy: 0.7607 - val_loss: 0.4598\n",
      "Epoch 6/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 10ms/step - accuracy: 0.9814 - loss: 0.0440 - val_accuracy: 0.9224 - val_loss: 0.1784\n",
      "Epoch 7/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 11ms/step - accuracy: 0.9820 - loss: 0.0427 - val_accuracy: 0.9782 - val_loss: 0.0633\n",
      "Epoch 8/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 10ms/step - accuracy: 0.9818 - loss: 0.0427 - val_accuracy: 0.9420 - val_loss: 0.2100\n",
      "Epoch 9/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 11ms/step - accuracy: 0.9816 - loss: 0.0426 - val_accuracy: 0.9738 - val_loss: 0.0681\n",
      "Epoch 10/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 11ms/step - accuracy: 0.9823 - loss: 0.0412 - val_accuracy: 0.9759 - val_loss: 0.0636\n",
      "Epoch 11/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 11ms/step - accuracy: 0.9824 - loss: 0.0409 - val_accuracy: 0.9780 - val_loss: 0.0546\n",
      "Epoch 12/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 11ms/step - accuracy: 0.9823 - loss: 0.0406 - val_accuracy: 0.9749 - val_loss: 0.0646\n",
      "Epoch 13/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 11ms/step - accuracy: 0.9825 - loss: 0.0402 - val_accuracy: 0.9755 - val_loss: 0.0669\n",
      "Epoch 14/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 11ms/step - accuracy: 0.9827 - loss: 0.0394 - val_accuracy: 0.9544 - val_loss: 0.1260\n",
      "Epoch 15/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 11ms/step - accuracy: 0.9826 - loss: 0.0397 - val_accuracy: 0.9795 - val_loss: 0.0522\n",
      "Epoch 16/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 10ms/step - accuracy: 0.9828 - loss: 0.0390 - val_accuracy: 0.8437 - val_loss: 0.4397\n",
      "Epoch 17/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 10ms/step - accuracy: 0.9826 - loss: 0.0392 - val_accuracy: 0.7257 - val_loss: 0.9574\n",
      "Epoch 18/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 10ms/step - accuracy: 0.9832 - loss: 0.0380 - val_accuracy: 0.9817 - val_loss: 0.0431\n",
      "Epoch 19/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 10ms/step - accuracy: 0.9835 - loss: 0.0370 - val_accuracy: 0.4979 - val_loss: 2.7105\n",
      "Epoch 20/20\n",
      "\u001b[1m13945/13945\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 10ms/step - accuracy: 0.9841 - loss: 0.0363 - val_accuracy: 0.9863 - val_loss: 0.0398\n"
     ]
    }
   ],
   "source": [
    "# Train the best model\n",
    "history = best_cnn_model.fit(X_train_reshaped, y_train_bal, epochs=20, batch_size=64, validation_split=0.3, callbacks=[early_stop], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15864/15864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.0367\n",
      "Test Accuracy: 0.9871\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "loss, accuracy = best_cnn_model.evaluate(X_test_reshaped, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15864/15864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2ms/step\n",
      "False Negatives (Attacks misclassified as Benign): 1235\n",
      "False Positives (Benign misclassified as Attacks): 5297\n",
      "\n",
      "False Negatives (Missed Attacks) - Example Indices: [ 164  418 1180 1309 1456]\n",
      "False Positives (Incorrectly Flagged Benign) - Example Indices: [ 27 133 152 295 374]\n",
      "[[415001   5297]\n",
      " [  1235  86107]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    420298\n",
      "           1       0.94      0.99      0.96     87342\n",
      "\n",
      "    accuracy                           0.99    507640\n",
      "   macro avg       0.97      0.99      0.98    507640\n",
      "weighted avg       0.99      0.99      0.99    507640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "y_pred = best_cnn_model.predict(X_test_reshaped)\n",
    "\n",
    "# Convert model probabilities to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# False Negative: Attack (1) misclassified as Benign (0)\n",
    "false_negatives = (y_pred_labels == 0) & (y_test_labels == 1)\n",
    "\n",
    "# False Positive: Benign (0) misclassified as Attack (1)\n",
    "false_positives = (y_pred_labels == 1) & (y_test_labels == 0)\n",
    "\n",
    "# Print summary\n",
    "print(f\"False Negatives (Attacks misclassified as Benign): {sum(false_negatives)}\")\n",
    "print(f\"False Positives (Benign misclassified as Attacks): {sum(false_positives)}\")\n",
    "\n",
    "# Show a few misclassified samples\n",
    "misclassified_fn = np.where(false_negatives)[0][:5]  # First 5 false negatives\n",
    "misclassified_fp = np.where(false_positives)[0][:5]  # First 5 false positives\n",
    "\n",
    "print(\"\\nFalse Negatives (Missed Attacks) - Example Indices:\", misclassified_fn)\n",
    "print(\"False Positives (Incorrectly Flagged Benign) - Example Indices:\", misclassified_fp)\n",
    "\n",
    "print(confusion_matrix(y_test_labels, y_pred_labels))\n",
    "print(classification_report(y_test_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "best_cnn_model.save('model.h5')\n",
    "best_cnn_model.save('model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
